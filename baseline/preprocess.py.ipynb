{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SQLContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=preprocess_data, master=local[*]) created by __init__ at <ipython-input-2-9c3e9459808c>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9c3e9459808c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preprocess_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rec_sys/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/miniconda3/envs/rec_sys/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    306\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 308\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    309\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=preprocess_data, master=local[*]) created by __init__ at <ipython-input-2-9c3e9459808c>:2 "
     ]
    }
   ],
   "source": [
    "conf = SparkConf().setAppName(\"preprocess_data\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = sqlContext.read.json(\"reviews_training_set.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_data.select(\"user_id\", \"business_id\", \"stars\")\n",
    "header = train_rdd.first() \n",
    "train_rdd = train_rdd.rdd.filter(lambda l: l!=header).map(lambda l: ((l[0],l[1]),(l[2],1))).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rdd = train_rdd.reduceByKey(lambda x,y:(x[0] + y[0],x[1] + y[1])).map(lambda l: (l[0],l[1][0]/l[1][1])).sortByKey(True).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('---1lKK3aKOuomHnwAkAow', '-ErwgUmZ1-jHW_rSu55jlg'), 5.0),\n",
       " (('---1lKK3aKOuomHnwAkAow', '1JgaRBX0oiRsvEhHF3ZMjw'), 1.0),\n",
       " (('---1lKK3aKOuomHnwAkAow', '2BbFeotL85cIaBjSq1SWiA'), 1.0),\n",
       " (('---1lKK3aKOuomHnwAkAow', '5cbsjFtrntUAeUx51FaFTg'), 1.0),\n",
       " (('---1lKK3aKOuomHnwAkAow', '5rxJpTkeJa5rxMvL2NbSnQ'), 5.0),\n",
       " (('---1lKK3aKOuomHnwAkAow', '78TC3sZSYBzBsSJ0z5pyhw'), 1.0),\n",
       " (('---1lKK3aKOuomHnwAkAow', 'Gaasy9YbPGVc8KcXcAIqEw'), 5.0),\n",
       " (('---1lKK3aKOuomHnwAkAow', 'HZdtHOEaKUL2SlWj5owgCA'), 5.0),\n",
       " (('---1lKK3aKOuomHnwAkAow', 'OnnvNs2JJ-B2xbwHEHbWDw'), 5.0),\n",
       " (('---1lKK3aKOuomHnwAkAow', 'PT9BRvRWx9kpcu02VouW1g'), 2.0)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_collect = train_rdd.collect()\n",
    "outfile = open(\"train_data.txt\",\"w\")\n",
    "for item in train_collect:\n",
    "    outfile.write(item[0][0] + \", \" + item[0][1] + \", \" + str(item[1]) + \"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = sqlContext.read.json(\"reviews_testing_set.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('---1lKK3aKOuomHnwAkAow', 'Qy_tDaVTWlS14fEglzo1Tg'), 4),\n",
       " (('---1lKK3aKOuomHnwAkAow', 'bzPrrbDHRxstf9FSPw1Xjw'), 5),\n",
       " (('---1lKK3aKOuomHnwAkAow', 'hk4NPN6W2xt30Cp-Fiqtuw'), 4),\n",
       " (('---1lKK3aKOuomHnwAkAow', 'rq5dgoksPHkJwJNQKlGQ7w'), 5),\n",
       " (('---1lKK3aKOuomHnwAkAow', 'ttH3ZbUcncBRIXqT-YVPCg'), 5),\n",
       " (('--CIuK7sUpaNzalLAlHJKA', 'sk0stgY4NDJYOX1MbNJ3Pg'), 2),\n",
       " (('--EMqnd727rtC0G5Oc-Mrg', 'b_XIKJ2nNzksuWhfMTEehQ'), 5),\n",
       " (('--FjmST55XwJ710qYlUTxA', 'pkXJEEaWcljXgAUd7cGzIA'), 1),\n",
       " (('--HCoE1ghaAlcaAfshICgw', 'caUPXiOxvLGMsorJBtNlhg'), 5),\n",
       " (('--IJ1MaFBp4bLFJ1xkofPw', 'ahSFUPojs9X3-1jP-QPb-w'), 5)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_rdd = test_data.select(\"user_id\", \"business_id\", \"stars\")\n",
    "test_rdd = test_rdd.rdd.map(lambda l:((l[0],l[1]),l[2])).sortByKey(True).persist()\n",
    "test_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_collect = test_rdd.collect()\n",
    "outfile = open(\"test_data.txt\",\"w\")\n",
    "for item in test_collect:\n",
    "    outfile.write(item[0][0] + \", \" + item[0][1] + \", \" + str(item[1]) + \"\\n\")\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
